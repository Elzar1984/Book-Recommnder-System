{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Crossing Recommendation System \n",
    "  \n",
    "Author: Eleni Zarogianni. \n",
    "October 2019. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: to implement a Book Recommender system that utilizes some sort of collaborative filtering using the online-available Book-Crossing Data set (http://www2.informatik.uni-freiburg.de/~cziegler/BX/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Data.\n",
    "\n",
    "The readily available Book Crossing Data set is used here. This dataset has been compiled by Cai-Nicolas Ziegler in 2004, and it comprises of three tables for users, books and ratings.\n",
    "BX-Users : Contains the users. User IDs (User-ID) have been anonymized and map to integers. Demographic data is provided (Location, Age) if available.\n",
    "BX-Books : Books are identified by their respective ISBN. Invalid ISBNs have already been removed from the dataset. Moreover, some content-based information is given (Book-Title, Book-Author, Year-Of-Publication, Publisher), obtained from Amazon Web Services. Note that in case of several authors, only the first is provided. URLs linking to cover images are also given, appearing in three different flavours (Image-URL-S, Image-URL-M, Image-URL-L), i.e., small, medium, large.\n",
    "BX-Book-Ratings : Contains the book rating information. Ratings (Book-Rating) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0.\n",
    "Let's jump straight into reading the csv files as pandas Dataframes (Dfs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory\n",
    "from os import chdir\n",
    "chdir('/Users/elenizarogianni/Desktop/EXUS_ML_Task')\n",
    "\n",
    "# Load Data \n",
    "# I've loaded them from my workspace!\n",
    "\n",
    "users = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "books = pd.read_csv('BX-Books.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "ratings = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Inspect and clean the data.\n",
    "\n",
    "In general, data inspection and cleaning prosedures include visually inspecting the data, through the use of graphs and plots, and figuring out any inconsistencies or peculiarities in the data sets. These might include, on a first-level, any duplicate entries or missing values, any wrongly assigned data types, and on a second-level any outliers. We will explore and handle each aspect of these below.\n",
    "\n",
    "Let's have a first glance at the data and check the Df's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the data\n",
    "print users.shape\n",
    "print books.shape\n",
    "print ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks fine. \n",
    "On itinial inspection, all 3 Df's contain column names with a '-'.That will lead to problems accessing the dataframes,so let's change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove middle slash  \n",
    "users.columns = ['userID', 'Location', 'Age']\n",
    "books.columns = ['ISBN', 'Title', 'Author', 'YearOfPublication', 'Publisher', 'ImageUrlS', 'ImageUrlM', 'imageUrlL']\n",
    "ratings.columns = ['userID', 'ISBN', 'BookRating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also on a first-look basis, we can already spot missing values (e.g. in the users.Age variable), but let's have a closer look and address each dataframe's idiosynchracies separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. USERS DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 5 first entries\n",
    "users.head(5)\n",
    "# Get basic info first.\n",
    "print users.info()\n",
    "# Describe numerical variables\n",
    "print users.describe()\n",
    "# Describe categorical variables\n",
    "print users.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First initial 5 entries confirm that we have missing values in Age. Also, data types of the Users' Df seem reasonable and therefore that's fine. \n",
    "\n",
    "Upon description of the Df, we can spot a 'weird' min-max duo for Age. We'll keep that in mind. The userID variable seems fine.\n",
    "\n",
    "Description of the Location, non-numerical variable seems OK but we can easily deduct that it might be more useful to split the Location variable into 3 separate ones, consisting of Town, State and Country that's more informationally relevant to a recommendation system.\n",
    "\n",
    "So let's get our hands on users.Location and users.Age variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location\n",
    "# Data cleaning (e.g. missing values, duplicates, data types problems and other inconisstencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print users.Location.isnull().any()\n",
    "# check for for duplicate entries\n",
    "users.Location.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split users.Location into 3 subparts\n",
    "location_expanded = users.Location.str.split(',', 2, expand=True)\n",
    "location_expanded.columns = ['Town', 'State', 'Country']\n",
    "users = users.join(location_expanded)\n",
    "# Drop the initial Location variable.\n",
    "users.drop('Location', axis=1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, Location has no missing values and there are non-unique entries (duplicates), which is certainly OK. We've splitted up into 3 sub-parts as described and dropped the initial, corresponding variable.\n",
    "\n",
    "Now, let's go an extra mile here, by having a look at some descriptives for location and some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, check on missing values and duplicates.\n",
    "print(users.Town.isnull().any())\n",
    "print(users.State.isnull().any())\n",
    "print(users.Country.isnull().any())\n",
    "\n",
    "# How many unique towns, states and countries do I have?\n",
    "nTowns=users.Town.nunique()\n",
    "nStates=users.Town.nunique()\n",
    "nCountries=users.Country.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} unique towns, {} unique states and {} unique coutries.\".format(nTowns, nStates, nCountries))\n",
    "print(\"In comparison to unique countries, total number of user entries is {}\".format(users.shape[0]))\n",
    "# users.userID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values for State and Country, which is problematic and requires to be dealt with. \n",
    "There are 32770 unique town and state entries, and 1276 unique coutries, in contrast to 278858 unique user entries (unique userIDs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing states and how many missing countries?\n",
    "print(users.State.isnull().sum())\n",
    "print(users.Country.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1 missing value for State and 2 for the Country variable. \n",
    "Let's do barplots for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State\n",
    "# Count number of users per each state\n",
    "states = users.State.value_counts()\n",
    "# Show the top 10 states based on their corresponding book users:\n",
    "users.State.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 States/Provinces per Book Users', alpha=.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A closer visual inspection of the states Df revealed other inconsistences too. For example, there are 'n/a' or '\\n/a\\\"'instances. I've also spotted a '.' instance, so there might as well exist other english stopwords. The best solution I think would be to throw all these instances in an 'Other' bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any instance of n/a, with 'other'\n",
    "print sum(users.State==' n/a') #  12527\n",
    "users['State'].replace(r'[\\s]n/a', 'other', regex = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any instances of '.' with 'other'.\n",
    "import string\n",
    "sum(users.State == ' .') # 15\n",
    "users['State'].replace(r'[\\s]\\.', 'other', inplace =True, regex= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty-string instances\n",
    "users.State.replace('', 'other', inplace=True)\n",
    "users.State.replace(' ', 'other', inplace=True)\n",
    "# OR users.State.replace('r[\\s]*', inplace=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there are some interesting double-letter or three-letter acronyms that my guess is they might correspond to US/other state or province acronyms. A visit to https://www.fs.fed.us/database/feis/format.html revealed the accuracy of my hunch for some of these, like the 'ca', 'nh', 'mi', 'df' etc. Others, like 'zh', 'sp' or 'rm' did not correspond to any of these states/provinces.\n",
    "\n",
    "I've downloaed the US/Canada province dictionary from here: http://code.activestate.com/recipes/577305-python-dictionary-of-us-states-and-territories/ and saved them all in py called provinces_mapping.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace state/province acronyms with their full names.\n",
    "from provinces_mapping import provinces\n",
    "# lower-case dictionary key-value pairs to match ours\n",
    "provinces = dict((k.lower(), v.lower()) for k,v in provinces.iteritems())\n",
    "\n",
    "# map the dictionary to the State column\n",
    "users['State'].map(provinces)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "print sum(users.State.isnull()) # 0\n",
    "# Replace Null values with 'other'\n",
    "users['State'] = users['State'].fillna('other')\n",
    "\n",
    "#plot again to observe differences.\n",
    "users.State.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 States/Provinces per Book Users', alpha=.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user.Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country\n",
    "# Count number of users per each country\n",
    "countries = users.Country.value_counts()\n",
    "# Show the top 10 countries according to their corresponding book users:\n",
    "users.Country.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 Countries per Book Users', alpha=.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USA is number one on books, with over 130.000 users, with Canada falling second with sixth below the amount of US. Interestingly, we observe in the 9th position an 'empty-string-country'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many countries are string-empty?\n",
    "print(users[users.Country == ''].Country.value_counts())\n",
    "# Replace empty string with 'Other' string.\n",
    "users.Country.replace('', 'other', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon closer inspection of the countries Df (series actually), we observe a bunch of inconsistencies with misplaced strings, such as: ',' ,'n/a', 'scotland/uk','uk,united kingdom', 'illinois, usa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for other inconsistencies\n",
    "# countries.Country contains the following inconsistences : ',' ,'n/a', 'scotland/uk','uk,united kingdom', 'illinois, usa'\n",
    "\n",
    "print sum(users.Country == ' n/a') #16\n",
    "users['Country'].replace(r'[\\s]n/a', 'other', regex = True, inplace=True) \n",
    "# Remove punctuation\n",
    "users['Country'].replace(r'[\\s][,.]', 'other', inplace =True, regex= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing Values\n",
    "sum(users.Country.isnull())\n",
    "users['Country'] = users['Country'].fillna('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "users.Town."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's know visit the Town variable and check things with this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users.Town\n",
    "# Count number of users per each state\n",
    "towns = users.Town.value_counts()\n",
    "# Show the top 10 states based on their corresponding book users:\n",
    "users.Town.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 Towns per Book Users', alpha=.70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print users.Town.nunique()   # 32770\n",
    "print(users.Town.isnull().sum()) # 0, which we remember from description above.\n",
    "\n",
    "# any n/a values?\n",
    "sum(users.Town == 'n/a')\n",
    "# Replace n/a with other\n",
    "users['Town'].replace(r'n/a', 'other', regex = True, inplace=True)\n",
    " \n",
    "# Replace punctuation marks with 'other'\n",
    "users['Town'].replace(r'[\\s]*[,.?]', 'other', inplace =True, regex= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "users.userID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the userID variable, although at first glance there wasn't anything wrong with it but let's double-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users.userID\n",
    "# Duplicate entries, missing entries, wrong data types of users.userID\n",
    "print users.userID.nunique()   # 278858\n",
    "print(users.userID.isnull().sum()) # 0, which we remember from description above.\n",
    "print users.userID.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, everything is OK with it. Let's now move into our numerical Variable, the Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users.Age\n",
    "# Describe variable\n",
    "users.Age.describe()\n",
    "# how many unique entries?\n",
    "print users.Age.nunique()   # 165\n",
    "\n",
    "# null/Nan values?\n",
    "print users.Age.isnull().any().sum() # 1\n",
    "# impute median after removing outliers\n",
    "print sum(np.isnan(users.Age)) ## 110762\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some null/Nan values that we will deal with later. Lets first do a distribution plot or histogram to further examine the age ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "sns.distplot(users.Age[~np.isnan(users.Age)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputes na values with the median\n",
    "medianAge = users.Age.median() \n",
    "users[\"Age\"].fillna(medianAge, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers detection\n",
    "ages = sorted(users.Age.unique())\n",
    "\n",
    "# histogram using 10-length bins, from 0-250 years.\n",
    "users.Age.hist(bins=range(0,250,10))\n",
    "plt.title('Age Distribution\\n')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the majority of users are between 20-40 years old, there are some over the age of 100, and some outliers falling on the edge of the graph, above 120-130 years (and up to 244), which is considered unreasonable.\n",
    "\n",
    "The best way in my opinion to deal with this is first do a scatterplot to figure out the amount of misplaced Age values, so that we can perhaps deduce whether this might be a systematic entry error or we can deduce any other useful insight. And then, probably go ahead with imputing these extreme values with the data's median (since the data are skewed, it's better suited than the average as a measure), or even the meadian of a meanigful's subset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scatterplot of users' ages.\n",
    "plt.scatter(users.Age, users.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very useful in the end. Let's narrow our investigation to people above the age of 110, which is a rather reasonable age for someone to stop reading(?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice users.Age variable and keep only values>110\n",
    "old_users = users.loc[(users.Age>110), 'Age']\n",
    "# how many are above 110?\n",
    "print old_users.shape \n",
    "# Create the Index \n",
    "index_ = list(range(1, 97)) \n",
    "# reset the index \n",
    "old_users.index = index_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot of users above the age of 110\n",
    "plt.scatter(old_users, index_)\n",
    "plt.show()\n",
    "# histogram\n",
    "plt.hist(old_users)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are only 96 cases of people above 100 yrs. According to the histogram about over 30 are between 110-120, and the rest span across the rest of the bins almost equally. \n",
    "\n",
    "So, we would better draw the line on the age of 120 and then impute the rest of the values to the data's median (which is the age of 32). This won't have a significant impact on the median, whereas if I chose to impute to another subset's median (maybe an older slot) that would probably skew my results a wee bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### impute median for extreme values, above 120 yrs\n",
    "# keep users above 120 yrs\n",
    "over120_users = users.loc[(users.Age>120), 'Age']\n",
    "# make a list of indexes to be replaced with the median\n",
    "list_indexes = over120_users.index\n",
    "# replace values with the median.\n",
    "users['Age'].replace(users.loc[users.index[list_indexes], 'Age'], medianAge, inplace =True)\n",
    "\n",
    "# sns.distplot(users.Age)\n",
    "# plt.hist(users.Age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. BOOKS Dataframe.\n",
    "\n",
    "Let's move to the Books dataframe now, and run initial descriptive analyses, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Books\n",
    "books.info()\n",
    "print books.dtypes\n",
    "\n",
    "#check for missing value\n",
    "books.isnull().any().any()\n",
    "\n",
    "# drop ImageUrls\n",
    "books.drop(['ImageUrlS', 'ImageUrlM', 'imageUrlL'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 271360 book entries. All book registries are of 'object' type. That should be altered for numerical-type variables, such as YearOfPublication.\n",
    "\n",
    "On first-level inspection, there are missing variables. We'll check later each variable separately and resolve this. Also, after going through a bit of search on the 'ImageUrl*' variables, it seems there's no additional value keeping them at the moment, so we dropped them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe variables\n",
    "print books.describe()\n",
    "# Check 5 first entries\n",
    "print books.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon dataframe's description, the variable Publisher seems to have missing entries. Also, first five entries are OK. Let's dive deeper into each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book.ISBN\n",
    "#check for missing value\n",
    "books.ISBN.isnull().any().any() #no missing value\n",
    "# Unique entries? \n",
    "books.ISBN.nunique() # check!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, everything OK with our key-identifier. Let's move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BooksTitle\n",
    "print books.Title.isnull().any() # False\n",
    "# check for unique entries\n",
    "print books.Title.nunique() # 242135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot top 10 book titles\n",
    "books.Title.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 Book titles', alpha=.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware that here we have top number of book titles, based on their ISBN numbers, not their usage. So, the above figure informs us that a signle book title may have more than 1 ISBNs associated with it (and that makes sense, if we consider different editions or formats of the book etc.)\n",
    "\n",
    "Let's take a closer look on the 'Little Women' book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print books[books.Title == 'Little Women']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 24 different ISBN for Little Women. Since, there's not available any relevant info for the different formats/editions/etc accounting for the different ISBNs, it would be most reasonable to assign unique ISBN to each and every single book Title because in the end we're interested in recommending a book, not a specific format/edition of it. I would however leave this unchanged for the time being and come back to it at a later point if there's time.\n",
    "\n",
    "Another solution that comes to mind though, would be to apply some sort of ISBN ML-clustering,based on the Book title. I will come to it later \n",
    "\n",
    "(Apart from the different ISBNs, right away we can observe problematic areas on the 'Author' variable, where there's 'Louisa May Alcott', 'Louisa M. Alcott' and 'Alcott', and also '0' values on the YearOf Publication. We will come later to those.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for other inconsistencies\n",
    "# not much (other than some special characters like '&'),\n",
    "# i will just convert everything to low-case, \n",
    "# in case there multiple entries, using a combo of small-capital letters.\n",
    "books.Title = books.Title.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BookAuthor\n",
    "# null entries\n",
    "print books.Author.isnull().any() # False\n",
    "# check for unique entries\n",
    "print books.Author.nunique() # 102024\n",
    "\n",
    "# plot top 10 book titles\n",
    "books.Author.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 Book Author', alpha=.70)\n",
    "\n",
    "# make lower-case\n",
    "books.Author = books.Author.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again here, no issues with missing values. We have 102024 unique Authors (lower-casing them resulted in the same number of unique entries). Top ten authors by their name show on the figure. No issue spotted on the figure, but we pointed out above, that some entries appear with their fullname, other their 'Name/Surname', and others with just their surname. We will try to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addressing Author name inconsistencies. \n",
    "\n",
    "# create a series for each unique book title\n",
    "un_bookTitles = pd.Series(books.Title.unique())\n",
    "\n",
    "for index, row in books.iterrows():\n",
    "    # for each book title, retrieve each entry\n",
    "    books_by_title = pd.DataFrame(books[books.Title == un_bookTitles[index]])\n",
    "    # for each book title, keep the lengthiest relevant author name and assign it back\n",
    "    list_uniq_authors = sorted(books_by_title.Author.unique(), reverse=True)\n",
    "    uniq_author = list_uniq_authors[0]\n",
    "    books_by_title.loc[index,'Author'] = uniq_author\n",
    "    # go to the books Df and equal for each author a single string describing him/her.\n",
    "    books.loc[index,'Author'] = books_by_title.loc[index, 'Author']\n",
    "    \n",
    "    books = books\n",
    "    \n",
    "return books\n",
    "\n",
    "#  !!! BEWARE \n",
    "# due to time-restriction, I did not include this in my dataset. So the books.Title variable\n",
    "# contains this name/surname, surname etc. inconsistencies.\n",
    "# I've left this part here however, as it runs just fine, it just takes a lot of time to finish,\n",
    "# and I could't affort it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware this operation is a bit time-consuming. Another solution would probably be to do a groupby operation, but tried it and was computationally-intensive for my machine (old MacBook problems!).   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Books.YearOfPublication\n",
    "print books.YearOfPublication.describe()\n",
    "\n",
    "# check for unique entries\n",
    "print books.YearOfPublication.nunique() # 202\n",
    "un_YofPub = pd.Series(sorted(books.YearOfPublication.unique())) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe two string values for the variable. We will remove them, and transform the pd.series to 'int64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  remove string variables\n",
    "books.YearOfPublication.replace(['DK Publishing Inc', 'Gallimard'], np.nan, inplace=True)\n",
    "\n",
    "# transform to int64 dtype. \n",
    "books.YearOfPublication.replace(np.nan,0, inplace=True)\n",
    "books.YearOfPublication = books.YearOfPublication.astype('int64')\n",
    "\n",
    "# check for nan/null values\n",
    "print books.YearOfPublication.isnull().sum() # 0\n",
    "\n",
    "# are there 0 entries ?\n",
    "zero_yr = books[books.YearOfPublication == 0].YearOfPublication.count()\n",
    "# replace them with NaN\n",
    "books.YearOfPublication.replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots for years of publication\n",
    "minYear = un_YofPub.min()\n",
    "maxYear = 2037\n",
    "\n",
    "books.YearOfPublication.hist(bins=range(minYear,2037,10))\n",
    "plt.title('Year Distribution\\n')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of books published')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some outliers before the year 1900 and after the year of 2000. Let's not also forget that according to the introduction, the Book-Crossing Library was created in 2004. There's a chance that some of the entries over 2004, are entry mistakes. Let's explore this a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book entries over 2004.\n",
    "books_over2004 = books[books.YearOfPublication>2005] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see among the entries, there are some clear mistakes, such as these of Edgar Allen Poe Collected Poems and Alice's Adventures in Wonderland and Through the Looking Glass (Puffin Books). In total there are 26 entries.\n",
    "\n",
    "I could cross-reference some of them and match them to the correct year of publication. But I would leave that, if there's time later.\n",
    "\n",
    "Let's move on for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Books.Publisher \n",
    "\n",
    "print books.Publisher.describe() #  271358\n",
    "\n",
    "# nan/null values\n",
    "print books.Publisher.isnull().sum() #  2\n",
    "# check for unique entries\n",
    "print books.Publisher.nunique() # 16807\n",
    "# lower-case the title.\n",
    "books.Publisher = books.Publisher.str.lower()\n",
    "# check again for unique entries\n",
    "print books.Publisher.nunique() # 16575\n",
    "\n",
    "\n",
    "# no empty strings in Publisher\n",
    "print(books[books.Publisher == ' '].Publisher.value_counts())\n",
    "\n",
    "# plot top 10 book titles\n",
    "books.Publisher.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 Publishers', alpha=.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedures are almost familiar to the reader now. There are 2 null entries, 16807 unique Publisher names. After lower-casing the names, this numbers drops down to 16575.\n",
    "A barplot showing the top-10 publishers is also given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Any other inconsistencies?\n",
    "un_publishers = pd.Series(books.Publisher.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspecting this variable doesn't reveal any eye-catching inconsistency. There's the chance however that the same publishers might be given slightly altered names due to entry mistakes. But this however would be time-consuming to explore, given that there might be no available corpus (or dictionary or whatever, as it was in the case of State/Provinces abbreviations) to map it onto.\n",
    "\n",
    "Let's go to our final Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. RATINGS Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings Dataframe\n",
    "# Describe  variables\n",
    "print ratings.describe()\n",
    "# Describe categorical variables\n",
    "print ratings.describe(include=['O'])\n",
    "\n",
    "# Check datatypes\n",
    "print ratings.dtypes\n",
    "\n",
    "# Check 5 first entries\n",
    "print ratings.head(5)\n",
    "\n",
    "# check for missing value\n",
    "print ratings.isnull().any().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings.userID\n",
    "# Duplicate entries, missing entries, wrong data types of users.userID\n",
    "print ratings.userID.nunique()   # 105283\n",
    "print ratings.userID.isnull().sum() # 0\n",
    "print ratings.userID.dtype # int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks OK and we move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings.ISBN\n",
    "#check for missing value\n",
    "print ratings.ISBN.isnull().any().any() # False\n",
    "# Unique entries? \n",
    "print ratings.ISBN.nunique() # 340556"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 340,556 unique ISBN, in a total of 1,149,780. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings.Rating\n",
    "# Describe variable\n",
    "ratings.Rating.describe()\n",
    "# how many unique entries?\n",
    "print ratings.Rating.nunique()   # 11\n",
    "print sorted(ratings.Rating.unique())  # 0 to 11\n",
    "\n",
    "# null/Nan values?\n",
    "print ratings.Rating.isnull().any().sum() # 0\n",
    "print sum(np.isnan(ratings.Rating)) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Join Tables and Finalize DataSet\n",
    "\n",
    "After cleaning each dataframe separately, we will merge the data together and get them ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE JOINS\n",
    "\n",
    "# join users with ratings on userID\n",
    "users_and_ratings = ratings.join(users.set_index('userID'), on='userID')\n",
    "print users_and_ratings.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data variables are joint and sizes are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-Checks!\n",
    "\n",
    "# users_and_ratings.userID\n",
    "print users_and_ratings.userID.isnull().sum()  # 0\n",
    "print users_and_ratings.userID.nunique()   # 105283\n",
    "print users_and_ratings.userID.dtype  #int64\n",
    "\n",
    "\n",
    "\n",
    "# users_and_ratings.ISBN\n",
    "#check for missing value\n",
    "print users_and_ratings.ISBN.isnull().any().any() # False\n",
    "# Unique entries? \n",
    "print users_and_ratings.ISBN.nunique() # 340556\n",
    "\n",
    "un_users_and_ratings_ISBN = pd.Series(sorted(users_and_ratings.ISBN.unique()))\n",
    "\n",
    "\n",
    "# users_and_ratings.Rating\n",
    "users_and_ratings.Rating.describe()\n",
    "# how many unique entries?\n",
    "print users_and_ratings.Rating.nunique()   # 11\n",
    "print sorted(users_and_ratings.Rating.unique())  # 0-10\n",
    "\n",
    "# null/Nan values?\n",
    "print users_and_ratings.Rating.isnull().any().sum() # 0\n",
    "print sum(np.isnan(users_and_ratings.Rating)) # 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eveything at this point seems OK. Let's do some standard double-checks for Town, State and Country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_and_ratings.Town\n",
    "# check for missing values\n",
    "print users_and_ratings.Town.isnull().any() #False \n",
    "# check for for duplicate entries\n",
    "print users_and_ratings.Town.nunique()  #16720\n",
    "\n",
    "# Count number of users per each country\n",
    "towns = users_and_ratings.Town.value_counts()\n",
    "\n",
    "# How many countries are string-empty?\n",
    "print(users_and_ratings[users_and_ratings.Town == ''].Town.value_counts())\n",
    "\n",
    "# Replace empty string with 'Other' string.\n",
    "users.Country.replace('', 'other', inplace=True)\n",
    "\n",
    "\n",
    "# any n/a values?\n",
    "print sum(users_and_ratings.Town== 'n/a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_and_ratings.State\n",
    "# same checks\n",
    "print users_and_ratings.State.isnull().any() # False\n",
    "# check for for duplicate entries\n",
    "print users_and_ratings.State.nunique() # 2508\n",
    "\n",
    "\n",
    "# Count number of users per each country\n",
    "states = users_and_ratings.State.value_counts()\n",
    "\n",
    "# How many countries are string-empty?\n",
    "print(users_and_ratings[users_and_ratings.State == ' '].Town.value_counts())\n",
    "\n",
    "\n",
    "# any n/a values?\n",
    "sum(users_and_ratings.State == 'n/a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_and_ratings.Country\n",
    "print users_and_ratings.Country.isnull().any() # False\n",
    "# check for for duplicate entries\n",
    "print users_and_ratings.Country.nunique() # 525\n",
    "\n",
    "\n",
    "# Count number of users per each country\n",
    "countries = users_and_ratings.Country.value_counts()\n",
    "\n",
    "# How many countries are string-empty?\n",
    "print(users_and_ratings[users_and_ratings.Country == ' '].Town.value_counts())\n",
    "\n",
    "# any n/a values?\n",
    "sum(users_and_ratings.Country == ' n/a')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, no problems here. We move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_and_ratings.Age\n",
    "# null/Nan values?\n",
    "print users_and_ratings.Age.isnull().any().sum() # 0\n",
    "# impute median after removing outliers\n",
    "print sum(np.isnan(users_and_ratings.Age)) ## 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All clear. Let's now Join this table to the books one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join in users_and_ratings with books on ISBN.\n",
    "users_and_ratings_and_books = users_and_ratings.join(books.set_index('ISBN'), on='ISBN')\n",
    "print users_and_ratings_and_books.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_and_ratings_and_books.ISBN\n",
    "#check for missing value\n",
    "print users_and_ratings_and_books.ISBN.isnull().any().any() # False\n",
    "# Unique entries? \n",
    "print users_and_ratings_and_books.ISBN.nunique() # 340556\n",
    "\n",
    "# un_users_and_ratings_and_books_ISBN = pd.Series(sorted(users_and_ratings_and_books.ISBN.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_and_ratings_and_books.Title\n",
    "# null entries\n",
    "print users_and_ratings_and_books.Title.isnull().any() # False\n",
    "# check for unique entries\n",
    "print users_and_ratings_and_books.Title.nunique() # 237912\n",
    "\n",
    "# plot top 10 book titles\n",
    "books.Title.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 Book titles', alpha=.70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_and_ratings_and_books.Author\n",
    "print users_and_ratings_and_books.Author.isnull().any() # False\n",
    "# check for unique entries\n",
    "print users_and_ratings_and_books.Author.nunique() # 98909\n",
    "\n",
    "# plot top 10 book titles\n",
    "users_and_ratings_and_books.Author.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 Book Author', alpha=.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GROUP BY OPERATIONS AND PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Recommendation system based on Collaborative Filtering\n",
    "\n",
    "In correlation based systems, recommendations are made based upon the similarity of the ratings/reviews given by users. \n",
    "\n",
    "So, for these systems, we use pearson correlation to suggest an item which is most similar to the item which user has already reviewed. In this sense, this technique takes user preference into account. If you want to refresh on Pearson correlation read here(https://datasciencebeginners.com/2018/09/30/05-statistics-and-branches-of-statistics-part-2/). Correlation based recommender systems are also called as item-based systems.\n",
    "Now let us see how to create correlation based recommendation system in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECOMMENDER SYSTEM\n",
    "# For an introduction to recommender systems, chech README.md and the EXUS_ML_REPORT.\n",
    "# Collaborative filtering: user-based CF and item-based CF\n",
    "# User-Item CF are based on the notion that \"Users who are similar to you also liked...\"    \n",
    "# Item-Item CF are based on :\"Users who liked this item also liked...\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create user-item matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Discussion \n",
    "\n",
    "\n",
    "In this notebook the 'Book-Crossing' dataset was used to create a recommendation system. \n",
    "\n",
    "A couple of CF-recommendation approaches were investigated, namely item-based and user-based CF methods.\n",
    "\n",
    "Of these, \n",
    "\n",
    "gave the best performance as assessed by comparing the predicted book ratings for a given user with the actual rating in a test set that the model was not trained on.\n",
    "\n",
    "\n",
    "The fields that were used for the model were the \"user ID\", \"book ID\", and \"rating\". There were others available in the dataset, such as \"age\", \"location\", \"publisher\", \"year published\", etc,\n",
    "\n",
    "\n",
    "Finally, we were able to build a recommender that could predict the 10 most likely book titles to be rated highly by a given user.\n",
    "\n",
    "\n",
    "It should be noted that this approach still suffers from the \"cold start problem\"[3] - that is, for users with no ratings or history the model will not make accurate predictions. One way we could tackle this problem may be to initially start with popularity-based recommendations, before building up enough user history to implement the model.\n",
    "\n",
    "Another piece of data that was not utilised in the current investigation was the \"implicit\" ratings - denoted as those with a rating of \"0\" in the dataset. Although more information about these implicit ratings (for example, does it represent a positive or negative interaction), these might be useful for supplementing the \"explicit\" ratings recommender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Enhancements - Future directions \n",
    "\n",
    "Below are some suggestions about directions/steps that could be taken in the future to further increase out recommendation model or even enhance the book recommedation system through use of other approaches.\n",
    "\n",
    "Also, there were other inconsistencies in the users. Even after cleaning the users.Town variable contained some name inconsistencies, such as values of 'c', 'b', 'ny', 'nis'. We could deduce a mapping for those, e.g. 'ny' probably refers to New York, but we couldn't do anything with single-letter ones. I decided to keep those entries and not remove them, so that I don't lose on data points.\n",
    "\n",
    "Some state mappings also remained unresolved, such as the 'rm' or 'sp'. In future, I might be able to map geographical data better as there are apis for states/provinces.\n",
    "\n",
    "Country variable contained some other inconsistencies that I didn't have the time to resolve. For example, some variables had entries like: 'scotland/uk' or 'uk/united kingdom'. This results in misleading entry numbers for each unique value (string) and therefore should be addressed.\n",
    " \n",
    "The afore-mentioned problem with non-unique ISBNs for the same book title (possibly corresponding to different editions or formats of the book) could be addressed at a later point.\n",
    " \n",
    "Also, some books appear to have years of publication above the year the dataset was released. Some of this, at it was mentioned in the corresponding section, are just entry mistakes that should be resurrected in the future.\n",
    " \n",
    "In the Book.Author variable, although I did implement a code snippet to account for name inconsistences regarding fullname or just surname, etc. inclusion, that wasn't included in the the final data set. \n",
    " \n",
    "Book title - description: another approach would be to search for book descriptions for every title and then implement so sort of Natural Language Processing (NLP) and bag-of-words approach to assign a category to every book title. For example, fiction, history, science etc. That would be usefule for recommending similar-category books based on rating. \n",
    " \n",
    "Implement a model-based CF approach, such as Singular Vector Decomposition(SVD) and User item matrix (Utility Matrix). These matrices contain data about ratings given by each user for each item. As all customers do not review each product, these matrices are mostly sparse.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. References \n",
    "\n",
    "I am a firm believer of transparency when building/implementing ML pipelines and an avid supporter of open-source coding and scientific research. Having said this, I must clarify that cold-blooded copy-pasting and plagiarism is something I frown upon, and this is very much instilled in me through my research years. \n",
    "\n",
    "To this end, I feel obliged to cite here a few of the wesites/blogs/papers/articles/books that helped with my understanding of the task and pushed my thinking even further. I hope you find them interesting and enlightening too! \n",
    " \n",
    "https://towardsdatascience.com/my-journey-to-building-book-recommendation-system-5ec959c41847 \n",
    " \n",
    "https://towardsdatascience.com/building-a-recommendation-system-for-fragrance-5b00de3829da \n",
    "https://github.com/kellypeng/scentmate_rec\n",
    "\n",
    "https://github.com/kellypeng/scentmate_rec\n",
    "\n",
    "https://datascienceplus.com/building-a-book-recommender-system-the-basics-knn-and-matrix-factorization/ \n",
    " \n",
    "https://github.com/tttgm/fellowshipai/blob/master/Book-Crossing-Recommender.ipynb \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Suggested Additional Reading\n",
    "\n",
    "Here are a couple of research papers, comparing and discussing CF-based and other recommendation frameworks on the Book-Crossing data set. \n",
    "\n",
    "https://pdfs.semanticscholar.org/ba2f/0b10f80ac3e569aef8a64320c54a4ca31e2b.pdf \n",
    "\n",
    "http://btw2017.informatik.uni-stuttgart.de/slidesandpapers/F-11-88/paper_web.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Closure\n",
    " \n",
    "Τhank you for taking the time to read through my Notebook. I would be happy to discuss my approach with you whenever it suits you. Looking forward to hearing back from you! \n",
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
