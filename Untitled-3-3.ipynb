{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Crossing Recommendation System \n",
    "  \n",
    "Author: Eleni Zarogianni \n",
    "October 2019 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: to implement a Book Recommender system that utilizes some sort of collaborative filtering using the online-available Book-Crossing Data set (http://www2.informatik.uni-freiburg.de/~cziegler/BX/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Data\n",
    "The readily available Book Crossing Data set is used here. This dataset has been compiled by Cai-Nicolas Ziegler in 2004, and it comprises of three tables for users, books and ratings.\n",
    "BX-Users : Contains the users. User IDs (User-ID) have been anonymized and map to integers. Demographic data is provided (Location, Age) if available.\n",
    "BX-Books : Books are identified by their respective ISBN. Invalid ISBNs have already been removed from the dataset. Moreover, some content-based information is given (Book-Title, Book-Author, Year-Of-Publication, Publisher), obtained from Amazon Web Services. Note that in case of several authors, only the first is provided. URLs linking to cover images are also given, appearing in three different flavours (Image-URL-S, Image-URL-M, Image-URL-L), i.e., small, medium, large.\n",
    "BX-Book-Ratings : Contains the book rating information. Ratings (Book-Rating) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0.\n",
    "Let's jump straight into reading the csv files as pandas Dataframes (Dfs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "books = pd.read_csv('BX-Books.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "ratings = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Inspect and clean the data \n",
    "In general, data inspection and cleaning prosedures include visually inspecting the data, through the use of graphs and plots, and figuring out any inconsistencies or peculiarities in the data sets. These might include, on a first-level, any duplicate entries or missing values, any wrongly assigned data types, and on a second-level any outliers. We will explore and handle each aspect of these below.\n",
    "\n",
    "Let's have a first glance at the data and check the Df's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the data\n",
    "print users.shape\n",
    "print books.shape\n",
    "print ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks fine. \n",
    "On itinial inspection, all 3 Df's contain column names with a '-'.That will lead to problems accessing the dataframes,so let's change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove middle slash  \n",
    "users.columns = ['userID', 'Location', 'Age']\n",
    "books.columns = ['ISBN', 'BookTitle', 'BookAuthor', 'YearOfPublication', 'Publisher', 'ImageUrlS', 'ImageUrlM', 'imageUrlL']\n",
    "ratings.columns = ['userID', 'ISBN', 'BookRating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also on a first-look basis, we can already spot missing values (e.g. in the users.Age variable), but let's have a closer look and address each dataframe's idiosynchracies separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USERS DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 5 first entries\n",
    "users.head(5)\n",
    "# Get basic info first.\n",
    "users.info()\n",
    "# Describe numerical variables\n",
    "users.describe()\n",
    "# Describe categorical variables\n",
    "users.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First initial 5 entries confirm that we have missing values in Age. Also, data types of the Users' Df seem reasonable and therefore that's fine. \n",
    "\n",
    "Upon description of the Df, we can spot a 'weird' min-max duo for Age. We'll keep that in mind. The userID variable seems fine.\n",
    "\n",
    "Description of the Location, non-numerical variable seems OK but we can easily deduct that it might be more useful to split the Location variable into 3 separate ones, consisting of Town, State and Country that's more informationally relevant to a recommendation system.\n",
    "\n",
    "So let's get our hands on users.Location and users.Age variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location\n",
    "# Data cleaning (e.g. missing values, duplicates, data types problems and other inconisstencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print users.Location.isnull().any()\n",
    "# check for for duplicate entries\n",
    "users.Location.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split users.Location into 3 subparts\n",
    "location_expanded = users.Location.str.split(',', 2, expand=True)\n",
    "location_expanded.columns = ['Town', 'State', 'Country']\n",
    "users = users.join(location_expanded)\n",
    "# Drop the initial Location variable.\n",
    "users.drop('Location', axis=1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, Location has no missing values and there are non-unique entries (duplicates), which is certainly OK. We've splitted up into 3 sub-parts as described and dropped the initial, corresponding variable.\n",
    "\n",
    "Now, let's go an extra mile here, by having a look at some descriptives for location and some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, check on missing values and duplicates.\n",
    "print(users.Town.isnull().any())\n",
    "print(users.State.isnull().any())\n",
    "print(users.Country.isnull().any())\n",
    "\n",
    "# How many unique towns, states and countries do I have?\n",
    "nTowns=users.Town.nunique()\n",
    "nStates=users.Town.nunique()\n",
    "nCountries=users.Country.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} unique towns, {} unique states and {} unique coutries.\".format(nTowns, nStates, nCountries))\n",
    "print(\"In comparison to unique countries, total number of user entries is {}\".format(users.shape[0]))\n",
    "# users.userID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values for State and Country, which is problematic and requires to be dealt with. \n",
    "There are 32770 unique town and state entries, and 1276 unique coutries, in contrast to 278858 unique user entries (unique userIDs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing states and how many missing countries?\n",
    "print(users.State.isnull().sum())\n",
    "print(users.Country.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1 missing value for State and 2 for the Country variable. \n",
    "Let's do barplots for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State\n",
    "# Count number of users per each state\n",
    "states = users.State.value_counts()\n",
    "# Show the top 10 states based on their corresponding book users:\n",
    "users.State.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 States/Provinces per Book Users', alpha=.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A closer visual inspection of the states Df revealed other inconsistences too. For example, there are 'n/a' or '\\n/a\\\"'instances. I've also spotted a '.' instance, so there might as well exist other english stopwords. The best solution I think would be to throw all these instances in an 'Other' bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any instance of n/a, with 'other'\n",
    "print sum(users.State==' n/a') #  12527\n",
    "users['State'].replace(r'[\\s]n/a', 'other', regex = True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any instances of '.' with 'other'.\n",
    "import string\n",
    "sum(users.State == ' .') # 15\n",
    "users['State'].replace(r'[\\s]\\.', 'other', inplace =True, regex= True)\n",
    "\n",
    "# # # #  OTHER PUNCTUATION SIGNS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty-string instances\n",
    "users.State.replace('', 'other', inplace=True)\n",
    "users.State.replace(' ', 'other', inplace=True)\n",
    "# OR users.State.replace('r[\\s]*', inplace=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there are some interesting double-letter or three-letter acronyms that my guess is they might correspond to US/other state or province acronyms. A visit to https://www.fs.fed.us/database/feis/format.html revealed the accuracy of my hunch for some of these, like the 'ca', 'nh', 'mi', 'df' etc. Others, like 'zh', 'sp' or 'rm' did not correspond to any of these states/provinces.\n",
    "\n",
    "I've downloaed the US/Canada province dictionary from here: http://code.activestate.com/recipes/577305-python-dictionary-of-us-states-and-territories/ and saved them all in py called provinces_mapping.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace state/province acronyms with their full names.\n",
    "from provinces_mapping import provinces\n",
    "# lower-case dictionary key-value pairs to match ours\n",
    "dict((k.lower(), v.lower()) for k,v in provinces.iteritems())\n",
    "\n",
    "# map the dictionary to the State column\n",
    "users['State'].map(provinces)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "sum(users.State.isnull()) # 1\n",
    "# Replace Null values with 'other'\n",
    "users['State'] = users['State'].fillna('other')\n",
    "\n",
    "\n",
    "#plot again to observe differences.\n",
    "users.State.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 States/Provinces per Book Users', alpha=.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What differences are there between first and last plot for State??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country\n",
    "# Count number of users per each country\n",
    "countries = users.Country.value_counts()\n",
    "# Show the top 10 countries according to their corresponding book users:\n",
    "users.Country.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 Countries per Book Users', alpha=.70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USA is number one on books, with over 130.000 users, with Canada falling second with sixth below the amount of US. Interestingly, we observe in the 9th position an 'empty-string-country'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many countries are string-empty?\n",
    "print(users[users.Country == ''].Country.value_counts())\n",
    "# Replace empty string with 'Other' string.\n",
    "users.Country.replace('', 'other', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon closer inspection of the countries Df (series actually), we observe a bunch of inconsistencies with misplaced strings, such as: ',' ,'n/a', 'scotland/uk','uk,united kingdom', 'illinois, usa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #   Checking for other inconsistencies\n",
    "# countries.Country contains the following inconsistences : ',' ,'n/a', 'scotland/uk','uk,united kingdom', 'illinois, usa'\n",
    "\n",
    "print sum(users.Country == ' n/a') #16\n",
    "users['Country'].replace(r'[\\s]n/a', 'other', regex = True, inplace=True) \n",
    "# Remove punctuation\n",
    "users['Country'].replace(r'[\\s][,.]', 'other', inplace =True, regex= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing Values\n",
    "sum(users.Country.isnull())\n",
    "users['Country'] = users['Country'].fillna('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # Checking for other inconsistencies, 'scotland/uk','uk,united kingdom', 'illinois, usa'\n",
    "\n",
    "\n",
    "\n",
    "# Groupby functions and Plots\n",
    "# groupby function  by location levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's know visit the Town variable and check things with this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Town\n",
    "# Count number of users per each state\n",
    "towns = users.Town.value_counts()\n",
    "# Show the top 10 states based on their corresponding book users:\n",
    "users.Town.value_counts()[:10].plot(kind='bar', stacked = 'True', title='Top 10 Towns per Book Users', alpha=.70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print users.Town.nunique()   # 32770\n",
    "print(users.Town.isnull().sum()) # 0, which we remember from description above.\n",
    "\n",
    "# any n/a values?\n",
    "sum(users.Town == 'n/a')\n",
    "# Replace n/a with other\n",
    "users['Town'].replace(r'n/a', 'other', regex = True, inplace=True)\n",
    " \n",
    "# Replace punctuation marks with 'other'\n",
    "users['Town'].replace(r'[\\s]*[,.?]', 'other', inplace =True, regex= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # # # # Checking for other inconsistencies, acronyms? 'c', b, ny, nis, !!!!!!!\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the userID variable, although at first glance there wasn't anything wrong with it but let's double-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users.userID\n",
    "#  Duplicate entries, missing entries, wrong data types of users.userID\n",
    "print users.userID.nunique()   # 278858\n",
    "print(users.userID.isnull().sum()) # 0, which we remember from description above.\n",
    "print users.userID.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, everything is OK with it.Let's now move into our numerical Variable, the Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe variable\n",
    "users.Age.describe()\n",
    "# how many unique entries?\n",
    "print users.Age.nunique()   # 165\n",
    "\n",
    "# null/Nan values?\n",
    "print users.Age.isnull().any().sum() # 1\n",
    "# impute median after removing outliers\n",
    "print sum(np.isnan(users.Age)) ## 110762\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some null/Nan values that we will deal with later. Lets first do a distribution plot or histogram to further examine the age ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "sns.distplot(users.Age[~np.isnan(users.Age)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers detection\n",
    "ages = sorted(users.Age.unique())\n",
    "\n",
    "# histogra. using 10-length bins, from 0-250 years.\n",
    "users.Age.hist(bins=range(0,250,10))\n",
    "plt.title('Age Distribution\\n')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the majority of users are between 20-40 years old, there are some over the age of 100, and some outliers falling on the edge of the graph, above 120-130 years (and up to 244), which is considered unreasonable.\n",
    "\n",
    "The best way in my opinion to deal with this is first, do a scatterplot to figure out the amount of misplaced Age values, so that we can perhaps deduce whether this might be a systematic error or we can assign some meaning to it. And then, probably go ahead with imputing these extreme values with the data's median, in the first case (since the data are skewed, it's better suited than the average as a measure), or even the meadian of a meanigful's subset, in the latter case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint plots of Age with Town/State/COuntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
