{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Crossing Recommendation System \n",
    "  \n",
    "Author: Eleni Zarogianni \n",
    "October 2019 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: to implement a Book Recommender system that utilizes some sort of collaborative filtering using the online-available Book-Crossing Data set (http://www2.informatik.uni-freiburg.de/~cziegler/BX/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Data\n",
    "The readily available Book Crossing Data set is used here. This dataset has been compiled by Cai-Nicolas Ziegler in 2004, and it comprises of three tables for users, books and ratings.  \n",
    " \n",
    "BX-Users : Contains the users. User IDs (`User-ID`) have been anonymized and map to integers. Demographic data is provided (`Location`, `Age`) if available. \n",
    "\n",
    "BX-Books : Books are identified by their respective ISBN. Invalid ISBNs have already been removed from the dataset. Moreover, some content-based information is given (`Book-Title`, `Book-Author`, `Year-Of-Publication`, `Publisher`), obtained from Amazon Web Services. Note that in case of several authors, only the first is provided. URLs linking to cover images are also given, appearing in three different flavours (`Image-URL-S`, `Image-URL-M`, `Image-URL-L`), i.e., small, medium, large. \n",
    " \n",
    "BX-Book-Ratings : Contains the book rating information. Ratings (`Book-Rating`) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0. \n",
    "\n",
    "Let's jump straight into reading the csv files as pandas Dataframes (Dfs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "books = pd.read_csv('BX-Books.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "ratings = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Inspect and clean the data\n",
    "In general, data inspection and cleaning prosedures include visually inspecting the data, through the use of graphs and plots, and figuring out any inconsistencies or peculiarities in the data sets. These might include, on a first-level, any duplicate entries or missing values, any wrongly assigned data types, and on a second-level any outliers. We will explore and handle each aspect of these below.\n",
    "\n",
    "Let's have a first glance at the data and check the Df's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the data\n",
    "print users.shape\n",
    "print books.shape\n",
    "print ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks fine.\n",
    "On itinial inspection, all 3 Df's contain column names with a '-'.That will lead to problems accessing the dataframes,so let's change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove middle slash  \n",
    "users.columns = ['userID', 'Location', 'Age']\n",
    "books.columns = ['ISBN', 'BookTitle', 'BookAuthor', 'YearOfPublication', 'Publisher', 'ImageUrlS', 'ImageUrlM', 'imageUrlL']\n",
    "ratings.columns = ['userID', 'ISBN', 'BookRating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also on a first-look basis, we can already spot missing values (e.g. in the users.Age variable), but let's have a closer look and address each dataframe's idiosynchracies separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USERS DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 5 first entries\n",
    "users.head(5)\n",
    "# Get basic info first.\n",
    "users.info()\n",
    "# Describe numerical variables\n",
    "users.describe()\n",
    "# Describe categorical variables\n",
    "users.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First initial 5 entries confirm that we have missing values in Age. Also, data types of the Users' Df seem reasonable and therefore that's fine. \n",
    "Upon description of the Df, we can sport a 'weird' min-max duo for Age. We'll keep that in mind. The userID variable seems fine.\n",
    "\n",
    "Description of the Location, non-numerical variable seems OK but we can easily deduct that it might be more useful to split the Location variable into 3 separate ones, consisting of Town, State and Country that's more informationally relevant to a recommendation system.\n",
    "\n",
    "So let's get our hands on users.Location and users.Age variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "users.Location.isnull().any()\n",
    "# check for for duplicate entries\n",
    "users.Location.nunique()\n",
    "\n",
    "# split users.Location into 3 subparts\n",
    "location_expanded = users.Location.str.split(',', 2, expand=True)\n",
    "location_expanded.columns = ['Town', 'State', 'Country']\n",
    "users = users.join(location_expanded)\n",
    "# Drop the initial Location variable.\n",
    "users.drop('Location', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, Location has no missing values and there are non-unique entries, which is certainly OK. We've splitted up into 3 sub-parts as described and dropped the initial, corresponding variable.\n",
    "\n",
    "Now, let's go an extra mile here, by having a look at some descriptives for location and some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique towns, states and countries do I have?\n",
    "users.Town.nunique()\n",
    "users.State.nunique()\n",
    "users.Country.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Users Otherwise, these fields contain NULL-values. \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
